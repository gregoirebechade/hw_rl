{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njfnT-sUER9J"
      },
      "source": [
        "# Lab on Stochastic Linear Bandits :\n",
        "\n",
        "We provide the environment to run a standard linear bandit experiment. The objective of this lab session is to understand how to implement LinUCB, the algorithm seen in class and its variant LinTS. We shall see that in practice there are some shortcomings in the implementation to make it efficient so we will guide you to obtain a working version.\n",
        "\n",
        "Questions are inline in the notebook and some reserved space are allocated for answers, but feel free to add cells for remarks and run your own experiments to test hypotheses you may have.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylZIq1mOJBr8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from scipy.stats import bernoulli\n",
        "from math import log\n",
        "\n",
        "import random\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "colors = sns.color_palette('colorblind')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_regret(regrets, logscale=False, lb=None,q=10):\n",
        "    \"\"\"\n",
        "    regrets must be a dict {'agent_id':regret_table}\n",
        "    \"\"\"\n",
        "\n",
        "    reg_plot = plt.figure()\n",
        "    #compute useful stats\n",
        "#     regret_stats = {}\n",
        "    for i, agent_id in enumerate(regrets.keys()):\n",
        "        data = regrets[agent_id]\n",
        "        N, T = data.shape\n",
        "        cumdata = np.cumsum(data, axis=1) # cumulative regret\n",
        "\n",
        "        mean_reg = np.mean(cumdata, axis=0)\n",
        "        q_reg = np.percentile(cumdata, q, axis=0)\n",
        "        Q_reg = np.percentile(cumdata, 100-q, axis=0)\n",
        "\n",
        "#         regret_stats[agent_id] = np.array(mean_reg, q_reg, Q_reg)\n",
        "\n",
        "        plt.plot(np.arange(T), mean_reg, color=colors[i], label=agent_id)\n",
        "        plt.fill_between(np.arange(T), q_reg, Q_reg, color=colors[i], alpha=0.2)\n",
        "\n",
        "    if logscale:\n",
        "        plt.xscale('log')\n",
        "        plt.xlim(left=100)\n",
        "\n",
        "    if lb is not None:\n",
        "        plt.plot(np.arange(T), lb, color='black', marker='*', markevery=int(T/10))\n",
        "\n",
        "    plt.xlabel('time steps')\n",
        "    plt.ylabel('Cumulative Regret')\n",
        "    plt.legend()\n",
        "    reg_plot.show()"
      ],
      "metadata": {
        "id": "X6gL_zi6LaZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3phj8yu79_Rt"
      },
      "source": [
        "# Environment Class\n",
        "\n",
        "The environment class allows to create 3 types of linear bandit problems:\n",
        "* 'fixed': normally requires a fixed_actions input (otherwise randomly generated at start) which is kept all along the game;\n",
        "* 'iid': at each round, the environment samples K actions at random on the sphere.\n",
        "\n",
        "For each of these types of game, the class is used to generate the action sets at each round and the reward for a chosen action (chosen by an Agent, see the \"Play!\" section for the details of the interaction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH892IKv95t3"
      },
      "source": [
        "### Action generators\n",
        "Please implement a function that generates K actions in dimension d. You may want to check the lecture slides to see whether some conditions should be respected.\n",
        "\n",
        "In the report, explain and justify your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKTTMwwB8rQS"
      },
      "outputs": [],
      "source": [
        "def ActionsGenerator(K,d, mean=None):\n",
        "    \"\"\"\n",
        "    K: int -- number of action vectors to be generated\n",
        "    d : int -- dimension of the action space\n",
        "    returns : an array of K vectors uniformly sampled on the unit sphere in R^d\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Bandit environment\n",
        "\n",
        "The following class is your environment: it generates an action set of K vectors at each round and returns the (random) reward given an action.\n",
        "You can see how it is used in the experiment function further below."
      ],
      "metadata": {
        "id": "jkWi1Qaiztia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKlD-rMUKumJ"
      },
      "outputs": [],
      "source": [
        "class LinearBandit:\n",
        "\n",
        "    def __init__(self, theta, K, var=1., fixed_actions=None):\n",
        "      \"\"\"\n",
        "      theta: d-dimensional vector (bounded) representing the hidden parameter\n",
        "      K: number of actions per round (random action vectors generated each time)\n",
        "      pb_type: string in 'fixed', 'iid', 'nsr' (please ignore NotSoRandom)\n",
        "      \"\"\"\n",
        "      self.d = np.size(theta)\n",
        "      self.theta = theta\n",
        "      self.K = K\n",
        "      self.var = var\n",
        "      self.current_action_set = np.zeros(self.d)\n",
        "\n",
        "\n",
        "\n",
        "    def get_action_set(self):\n",
        "      \"\"\"\n",
        "      Generates a set of vectors in dimension self.d. Use your ActionsGenerator\n",
        "      Alternatively, the set of actions is fixed a priori (given as input).\n",
        "      Implement a condition to return the fixed set when one is given\n",
        "      \"\"\"\n",
        "      return np.eye(self.d) ## dummy return, it only returns d actions (TODO)\n",
        "\n",
        "\n",
        "    def get_reward(self, action):\n",
        "      \"\"\" sample reward given action and the model of this bandit environment\n",
        "      action: d-dimensional vector (action chosen by the learner)\n",
        "      \"\"\"\n",
        "      mean = np.dot(action, self.theta)\n",
        "      return np.random.normal(mean, scale=self.var)\n",
        "\n",
        "    def get_means(self):\n",
        "      return np.dot(self.current_action_set, self.theta)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73lNhpBGwxFx"
      },
      "source": [
        "# Play !\n",
        "The function play runs one path of regret for one agent. The function experiment runs all agents several (Nmc) times and returns all the logged data. Feel free to check the inputs and outputs required when you decide on the implementation of your own agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChN6oiILAk2S"
      },
      "outputs": [],
      "source": [
        "def play(environment, agent, Nmc, T, pseudo_regret=True):\n",
        "    \"\"\"\n",
        "    Play one Nmc trajectories over a horizon T for the specified agent.\n",
        "    Return the agent's name (sring) and the collected data in an nd-array.\n",
        "    \"\"\"\n",
        "\n",
        "    data = np.zeros((Nmc, T))\n",
        "\n",
        "\n",
        "\n",
        "    for n in range(Nmc):\n",
        "        agent.reset()\n",
        "        for t in range(T):\n",
        "            action_set = environment.get_action_set()\n",
        "            action = agent.get_action(action_set)\n",
        "            reward = environment.get_reward(action)\n",
        "            agent.receive_reward(action,reward)\n",
        "\n",
        "            # compute instant (pseudo) regret\n",
        "            means = environment.get_means()\n",
        "            best_reward = np.max(means)\n",
        "            if pseudo_regret:\n",
        "              # pseudo-regret removes some of the noise and corresponds to the metric studied in class\n",
        "              data[n,t] = best_reward - np.dot(environment.theta,action)\n",
        "            else:\n",
        "              data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive\n",
        "\n",
        "    return agent.name(), data\n",
        "\n",
        "\n",
        "def experiment(environment, agents, Nmc, T,pseudo_regret=True):\n",
        "    \"\"\"\n",
        "    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    all_data = {}\n",
        "\n",
        "    for agent in agents:\n",
        "        agent_id, regrets = play(environment, agent,Nmc, T,pseudo_regret)\n",
        "\n",
        "        all_data[agent_id] = regrets\n",
        "\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_PSPRfU-CRy"
      },
      "source": [
        "# Linear Bandit Agents\n",
        "\n",
        "> Ajouter une citation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny1zhj6NwxFy"
      },
      "source": [
        "\n",
        "## LinUCB : Implementing optimism in $R^d$\n",
        "\n",
        "As seen in class, the actions are now vectors in $R^d$, representing contextual features, and the environment is assumed to generate rewards according to some hidden linear function $f_\\theta(a) = a^\\top \\theta$. The goal of the learner is thus to estimate $\\theta$ while keeping a measure of the uncertainty in all the directions of the feature space.\n",
        "\n",
        "* **Baseline: Implementation of LinEpsilonGreedy** In the next cell, we implemented a LinUniform Agent that returns one of the action vectors of the action set, chosen uniformly at random. Please implement a `LinEpsilonGreedy` agent and test it against `Greedy` ($\\epsilon=0$) on the 2 proposed  environments (iid and fixed actions). What do you notice? Is $\\epsilon$-Greedy a good baseline to test algorithms?\n",
        "\n",
        "\n",
        "* **Implementation of LinUCB**: you need to compute UCBs for each arm of the current action set received from the environment, but this time the exploration bonus depends on the history of taken actions and received rewards (see course material).\n",
        "\n",
        "* **Efficiency of the matrix inversion step**: One key step is to invert the covariance matrix in order to compute the elliptical norm of each available action. Remark however that at round $t+1$, the new covariance matrix is very similar to the previous one at rount $t$... Can you think of a way to optimize this step by simply updating the old one ? Please implement this improvement as an additional option to your `LinUCB` agent so you can compare runtimes from bandit problems in dimension $d=2,8,16,32,64$. Plot the result of the compared runtimes. In your report,  discuss the computational complexity of this step and the resulting improvement, give your new update formula, and report the plot of compared runtimes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2qQCH4_wxFy"
      },
      "source": [
        "### Uniform random policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9uEOaPwBAbA"
      },
      "outputs": [],
      "source": [
        "class LinUniform:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def get_action(self, arms):\n",
        "    K, _ = arms.shape\n",
        "    return arms[np.random.choice(K)]\n",
        "\n",
        "  def receive_reward(self, chosen_arm, reward):\n",
        "    pass\n",
        "\n",
        "  def reset(self):\n",
        "    pass\n",
        "\n",
        "  def name(self):\n",
        "    return 'Unif'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SCdWRH0wxFy"
      },
      "source": [
        "### Lin-$\\epsilon$-Greedy policy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC8Zp87qwxFy"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import pinv\n",
        "\n",
        "class LinEpsilonGreedy:\n",
        "  def __init__(self, d,lambda_reg, eps=0.1, other_option=None):\n",
        "    self.eps = eps # exploration probability\n",
        "    self.d = d\n",
        "    self.lambda_reg = lambda_reg\n",
        "    self.reset()\n",
        "    #use other inputs if needed\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"\n",
        "    This function should reset all estimators and counts.\n",
        "    It is used between independent experiments (see 'Play!' above)\n",
        "    \"\"\"\n",
        "    self.t = 0\n",
        "    self.hat_theta = np.zeros(self.d)\n",
        "    self.cov = self.lambda_reg * np.identity(self.d)\n",
        "    self.invcov = np.identity(self.d)\n",
        "    self.b_t = np.zeros(self.d)\n",
        "\n",
        "  def get_action(self, arms):\n",
        "    K, _ = arms.shape\n",
        "    return arms[0,:] ##dummy return: TODO\n",
        "\n",
        "  def receive_reward(self, chosen_arm, reward):\n",
        "    \"\"\"\n",
        "    update the internal quantities required to estimate the parameter theta using least squares\n",
        "    \"\"\"\n",
        "\n",
        "    #update inverse covariance matrix\n",
        "    self.invcov = np.eye(self.d) #dummy instantiation: TODO\n",
        "\n",
        "\n",
        "    #update b_t\n",
        "    self.b_t += reward * chosen_arm\n",
        "\n",
        "    self.hat_theta = np.inner(self.invcov, self.b_t) # update the least square estimate\n",
        "    self.t += 1\n",
        "\n",
        "  def name(self):\n",
        "    return 'LinEGreedy('+str(self.eps)+')'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Default setting\n",
        "\n",
        "Quick test: I propose some simple settings to run a quick test.\n",
        "For your report, feel free to change these and explain / justify your choices.\n",
        "Please report the resulting plot and your conclusions on Lin-E-Greedy."
      ],
      "metadata": {
        "id": "PRY4RXMo1nUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = 3  # dimension\n",
        "K = 7  # number of arms\n",
        "\n",
        "# parametor vector \\theta, normalized :\n",
        "# theta = ActionsGenerator(1,d) # another way of getting a 'random vector' (depends on your implementation)\n",
        "theta = np.array([0.45, 0.5, 0.5])\n",
        "theta /= np.linalg.norm(theta)\n",
        "\n",
        "T = 100  # Finite Horizon\n",
        "N = 10  # Monte Carlo simulations\n",
        "\n",
        "delta = 0.1 # could be set directly in the algorithms\n",
        "sigma = 1.\n",
        "\n",
        "#choice of percentile display\n",
        "q = 10\n"
      ],
      "metadata": {
        "id": "UUKH70ENPUMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = LinearBandit(theta, K, var=sigma**2)"
      ],
      "metadata": {
        "id": "Rye2g9lgQHD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# policies\n",
        "\n",
        "uniform = LinUniform()\n",
        "e_greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.1)\n",
        "greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.)"
      ],
      "metadata": {
        "id": "-Fa2aJV7SIze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp = experiment(env, [greedy, e_greedy, uniform], Nmc=N, T=T,pseudo_regret=True)\n",
        "plot_regret(exp)"
      ],
      "metadata": {
        "id": "MCaCjjshSNse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAVBQ2k5wxF0"
      },
      "source": [
        "Is Lin-E-Greedy a strong baseline? Is it hard to beat?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUyi4C_OwxFy"
      },
      "source": [
        "## Lin-UCB: The optimistic way\n",
        "\n",
        "**Implement LinUCB** as seen in class and test it against the baselines implemented above.\n",
        "\n",
        "If you are happy with the result, move to the last part below.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#class LinUCB: ... (use the template of LinEGreedy and modify the get_action function)"
      ],
      "metadata": {
        "id": "sVfDJvY4LCSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPSEMvWhwxF1"
      },
      "source": [
        "### Test against baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqGTqj2-wxFz"
      },
      "outputs": [],
      "source": [
        "# policies\n",
        "\n",
        "linucb = LinUCB(d, delta, sigma=sigma, lambda_reg=1.)\n",
        "uniform = LinUniform()\n",
        "e_greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.1)\n",
        "greedy = LinEpsilonGreedy(d, lambda_reg=1., eps=0.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jej__TQwwxF1"
      },
      "outputs": [],
      "source": [
        "linucb_vs_greedy = experiment(iid_env, [greedy, e_greedy, linucb], Nmc=N, T=T,pseudo_regret=True)\n",
        "plot_regret(linucb_vs_greedy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p_flT3gwxFz"
      },
      "source": [
        "## LinTS : Taking the Bayesian way\n",
        "\n",
        "Thompson Sampling is a popular bayesian alternative to the standard optimistic bandit algorithms (see Chapter 36 of Bandit Algorithms). The key idea is to rely on Bayesian *samples* to get a proxy for the hidden parameter $\\theta$ of the problem instead of building high-probability confidence regions.\n",
        "\n",
        "* **Posterior derivation**: Let us place a Gaussian prior with mean $\\mathbf{0}$ and covariance $\\lambda I$ on $\\theta$. Given actions $A_1,\\ldots,A_t$ and rewards $Y_1,\\ldots,Y_t$, Can you compute the expression of the posterior at the beginning of round $t+1$ ?\n",
        "\n",
        "In your report, write the distribution of the posterior as a function of the prior and the observed data. No need to report your full derivation if you are lacking space.\n",
        "\n",
        "\n",
        "* **Implementation of LinTS**. Please implement Linear Thompson Sampling using the formula you derived above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_navr3GuwxFz"
      },
      "outputs": [],
      "source": [
        "# class LinTS ... (use you LinUCB template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t2LC5HFwxFz"
      },
      "source": [
        "### Comparison and report\n",
        "\n",
        "Compare LinUCB, LinTS and LinEGreedy on a problem of your choice. In your report, explain your choice of problem and report the plot of your experiment as well as a few sentences of comment: Is there a clear 'winner'?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus\n",
        "\n",
        "In this bonus part, we explore the role of the action sets on the performance of our algorithms.\n",
        "\n",
        "In class we said that action sets can be 'arbitrary'. This means that, in principle, they do not have to follow a distribution, they do not have to be random either.\n",
        "\n",
        "What happens if the action set is fixed?\n",
        "\n",
        "We propose an alternative 'play' function that fixes the action set:"
      ],
      "metadata": {
        "id": "NIXTRA9UNn5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def play_fixed(environment, agent, Nmc, T, actions=None, pseudo_regret=True):\n",
        "    \"\"\"\n",
        "    Play one Nmc trajectories over a horizon T for the specified agent.\n",
        "    Return the agent's name (sring) and the collected data in an nd-array.\n",
        "    actions: a fixed action set. Default is set to be the canonical basis.\n",
        "    \"\"\"\n",
        "\n",
        "    data = np.zeros((Nmc, T))\n",
        "\n",
        "    for n in range(Nmc):\n",
        "        agent.reset()\n",
        "        for t in range(T):\n",
        "            # action_set = environment.get_action_set() -> We no longer call on your ActionsGenerator function\n",
        "            action_set = np.copy(actions) # the actions given as input\n",
        "            action = agent.get_action(action_set)\n",
        "            reward = environment.get_reward(action)\n",
        "            agent.receive_reward(action,reward)\n",
        "\n",
        "            # compute instant (pseudo) regret\n",
        "            means = environment.get_means()\n",
        "            best_reward = np.max(means)\n",
        "            if pseudo_regret:\n",
        "              data[n,t] = best_reward - np.dot(environment.theta,action)\n",
        "            else:\n",
        "              data[n,t]= best_reward - reward # this can be negative due to the noise, but on average it's positive\n",
        "\n",
        "    return agent.name(), data"
      ],
      "metadata": {
        "id": "6gAnyXJnOHJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def experiment_fixed(environment, agents, Nmc, T, actions=None, pseudo_regret=True):\n",
        "    \"\"\"\n",
        "    Play Nmc trajectories for all agents over a horizon T. Store all the data in a dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    all_data = {}\n",
        "    if actions is None:\n",
        "      actions = np.ActionsGenerator(K,d) #call it once!\n",
        "    print(actions)\n",
        "\n",
        "    for agent in agents:\n",
        "        agent_id, regrets = play_fixed(environment, agent, Nmc, T, actions=actions, pseudo_regret=pseudo_regret)\n",
        "\n",
        "        all_data[agent_id] = regrets\n",
        "\n",
        "    return all_data"
      ],
      "metadata": {
        "id": "rz1x0DhNO97W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the actions are fixed, so we could actually use UCB to address the problem: after all, it is just a K-armed bandit, but with structure.\n",
        "\n",
        "**When is LinUCB better than UCB?**"
      ],
      "metadata": {
        "id": "jCQ8xr5NO-pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UCB:\n",
        "  def __init__(self, K, var):\n",
        "      self.K = K\n",
        "      self.var = var\n",
        "      self.count_actions = np.zeros(self.K)\n",
        "      self.count_rewards = np.zeros(self.K)\n",
        "      self.t = 0\n",
        "\n",
        "  def get_action(self,action_set):\n",
        "      if self.t < self.K:\n",
        "        action = self.t\n",
        "      else:\n",
        "        empirical_means = self.count_rewards / self.count_actions\n",
        "        ucbs = np.sqrt(6 * self.var * np.log(self.t) / self.count_actions) # 6 could be replaced by a 2, try it out :)\n",
        "        action = np.argmax(empirical_means + ucbs)\n",
        "\n",
        "      self.t += 1\n",
        "      self.count_actions[action] += 1\n",
        "      self.current_action = action #need to remember the *index* of the action now\n",
        "      return action_set[action,:]\n",
        "\n",
        "  def receive_reward(self, action, reward):\n",
        "      self.count_rewards[self.current_action] += reward\n",
        "\n",
        "  def reset(self):\n",
        "      self.count_actions = np.zeros(self.K)\n",
        "      self.count_rewards = np.zeros(self.K)\n",
        "      self.t = 0\n",
        "\n",
        "  def name(self):\n",
        "      return 'UCB('+str(self.var)+')'"
      ],
      "metadata": {
        "id": "xtRBD1QFPrcN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d=3\n",
        "K=7 # same as before, more actions than dimension\n",
        "\n",
        "env = LinearBandit(theta, K)\n",
        "ucb = UCB(K,var=1.)\n",
        "linucb = LinUCB(d, delta=0.01, lambda_reg=1., sigma=1. )\n",
        "reg_fixed_actions = experiment_fixed(env, [ucb, linucb], Nmc=10, T=200, actions=None, pseudo_regret=True)"
      ],
      "metadata": {
        "id": "R1rf1eYyRzJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_regret(reg_fixed_actions)"
      ],
      "metadata": {
        "id": "-4fjn4FrSXgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, LinUCB is better in such problem because it shares information across actions.\n",
        "\n",
        "Now, what if K=d and the fixed action set is exactly the canonical basis (K=d independent actions)"
      ],
      "metadata": {
        "id": "QD3Si1xIWJG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d=7\n",
        "K=d\n",
        "# theta = np.random.multivariate_normal(np.zeros(d),np.eye(d))\n",
        "theta = np.linspace(0.1,1,num=d) # just d actions in increasing value order\n",
        "#theta = your choice of parameter\n",
        "theta /= np.linalg.norm(theta) #optional if you set theta with bounded norm :)\n",
        "\n",
        "env = LinearBandit(theta, d, fixed_actions=np.eye(d))\n",
        "ucb = UCB(d,var=1.)\n",
        "linucb = LinUCB(d, delta=0.01, lambda_reg=1., sigma=1. )\n",
        "reg_fixed_actions = experiment_fixed(env, [ucb, linucb], Nmc=10, T=200, actions=np.eye(d), pseudo_regret=True)"
      ],
      "metadata": {
        "id": "ME-Hn3ryVb8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_regret(reg_fixed_actions)"
      ],
      "metadata": {
        "id": "BokgjamhbhtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two algorithms should be roughly on par. In fact, it is possible to refine UCB to get slightly better performance (by tightening a bit the upper confidence bounds).\n",
        "\n",
        "In \"[The End of Optimism](https://arxiv.org/pdf/1610.04491)\", Lattimore et al. show that for a certain type of action set, one can show that UCB has **linear regret**. A famous 'counter-example' (bad action set) is given in Section 8 therein.\n",
        "\n",
        "**Exercise**: find the action set (3 actions in dimension 2) and run the experiment above, what do you see? In (the appendix of) your report, please report your figures and conclusions.\n",
        "\n",
        "The problem of finding a good algorithm for the 'arbitrary' setting and the 'fixed-design' setting was open for a long time but recent papers (e.g. [Kirschner et al.,2021](https://arxiv.org/abs/2011.05944)) have now proposed solutions."
      ],
      "metadata": {
        "id": "mTZPar2nbjqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9hoJXs4Ldw-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}